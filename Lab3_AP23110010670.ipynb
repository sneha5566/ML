{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5d3ee0",
   "metadata": {},
   "source": [
    "# üéØ Feature Engineering ‚Äî Part A: Individual Concepts (Colab-Ready)\n",
    "\n",
    "**Updated:** 2025-08-22\n",
    "\n",
    "This notebook is designed for **first-time learners**. You will practice each feature engineering step **individually** (no pipelines yet), so you can clearly see *what each step does* and *why it matters*.\n",
    "\n",
    "**What you'll practice:**\n",
    "- Dataset loading & quick audit\n",
    "- Handling missing values (drop, impute)\n",
    "- Scaling & normalization (standardization, min-max, per-row normalization)\n",
    "- Encoding categorical variables (ordinal vs one-hot)\n",
    "- Feature transformations (log, power, polynomial)\n",
    "- Simple dimensionality reduction (PCA) for visualization\n",
    "- Short exercises after each section\n",
    "\n",
    "> Use this Part A first. After you are comfortable, move to **Part B (Pipelines)** to automate and combine steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04499a3f",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Google Colab, you can install optional packages here:\n",
    "# !pip install -q statsmodels==0.14.2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de06ce7",
   "metadata": {},
   "source": [
    "## 1) Dataset Setup & Quick Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load Titanic from a stable GitHub mirror (recommended for first run)\n",
    "URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6588cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Upload your own CSV (uncomment to use in Colab)\n",
    "# from google.colab import files\n",
    "# up = files.upload()  # pick file\n",
    "# import io\n",
    "# df = pd.read_csv(io.BytesIO(up[list(up.keys())[0]]))\n",
    "# print(\"Shape:\", df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick audit\n",
    "print(\"\\nInfo:\")\n",
    "df.info()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "print(\"\\nNumeric describe:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84781f19",
   "metadata": {},
   "source": [
    "## 2) Handling Missing Values (Individually)\n",
    "\n",
    "**Goal:** Learn when to **drop** vs **impute**.\n",
    "\n",
    "**Common choices**\n",
    "- Numeric: mean/median\n",
    "- Categorical: most frequent\n",
    "\n",
    "We'll practice on Titanic columns like `Age`, `Embarked`, and `Cabin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View null counts\n",
    "df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 DROP example (use cautiously)\n",
    "df_drop_rows = df.dropna(subset=['Age', 'Embarked'])  # drop rows where these are null\n",
    "print(\"Original:\", df.shape, \"After drop:\", df_drop_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 SIMPLE IMPUTE example\n",
    "df_imp = df.copy()\n",
    "# Numeric (Age): median\n",
    "df_imp['Age'] = df_imp['Age'].fillna(df_imp['Age'].median())\n",
    "# Categorical (Embarked): most frequent\n",
    "df_imp['Embarked'] = df_imp['Embarked'].fillna(df_imp['Embarked'].mode()[0])\n",
    "\n",
    "# 'Cabin' is very sparse; we can fill with \"Unknown\"\n",
    "df_imp['Cabin'] = df_imp['Cabin'].fillna('Unknown')\n",
    "\n",
    "df_imp.isna().sum().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 KNN Imputation (numeric only demonstration)\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "knn_df = df[num_cols].copy()\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "knn_imputed = imputer.fit_transform(knn_df)\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed, columns=num_cols)\n",
    "knn_imputed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902aff5",
   "metadata": {},
   "source": [
    "**üìù Exercise 2**\n",
    "1) Compare **mean vs median** imputation for `Age`. Which preserves the original distribution better?  \n",
    "2) For `Embarked`, try filling with a new category (`'Unknown'`) vs mode. What changes in `value_counts()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dadca1",
   "metadata": {},
   "source": [
    "Filling with Mode\n",
    "df['Embarked_mode'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "Most frequent category (likely \"S\") fills the gaps.\n",
    "value_counts() ‚Üí \"S\" increases, distribution slightly biased toward majority class.\n",
    "\n",
    "Filling with 'Unknown'\n",
    "df['Embarked_unknown'] = df['Embarked'].fillna('Unknown')\n",
    "Keeps missingness visible as a separate category.\n",
    "value_counts() ‚Üí new \"Unknown\" category appears.\n",
    "Preserves original frequency counts for \"C\", \"Q\", \"S\".\n",
    "Useful for models that can handle categorical variables, as \"Unknown\" may carry information.\n",
    "\n",
    "\n",
    "Mode filling inflates the count of the most frequent port (bias).\n",
    "'Unknown' filling adds a new category, preserving the original class proportions while making missingness explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1bf99",
   "metadata": {},
   "source": [
    "Mean Imputation\n",
    "df['Age_mean'] = df['Age'].fillna(df['Age'].mean())\n",
    "Replaces missing ages with the average of all available ages.\n",
    "Problem: The mean is sensitive to outliers (e.g., very high ages like 80‚Äì90).\n",
    "This tends to shift the distribution towards the center, reducing variability.\n",
    "Histogram after mean imputation ‚Üí a spike near the mean.\n",
    "\n",
    "Median Imputation\n",
    "df['Age_median'] = df['Age'].fillna(df['Age'].median())\n",
    "Replaces missing ages with the middle value.\n",
    "More robust to skewness and outliers.\n",
    "The distribution shape is preserved better compared to mean.\n",
    "Histogram after median imputation ‚Üí spike at the median, but less distortion.\n",
    "\n",
    "Median imputation preserves the original distribution better because it‚Äôs less affected by skewness and extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba27c3",
   "metadata": {},
   "source": [
    "## 3) Scaling & Normalization (Individually)\n",
    "\n",
    "- **Standardization**: z = (x - mean)/std (good for many ML models)\n",
    "- **MinMax scaling**: maps to [0,1] (useful when features have different units)\n",
    "- **Per-row Normalization**: scales each *row vector* to unit norm (useful for text-like frequency vectors)\n",
    "\n",
    "We'll demonstrate on `Fare` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(df_imp['Age'].dropna(), bins=30)\n",
    "axes[0].set_title('Age - Raw')\n",
    "axes[1].hist(df_imp['Fare'].dropna(), bins=30)\n",
    "axes[1].set_title('Fare - Raw')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_std = StandardScaler()\n",
    "sc_mm  = MinMaxScaler()\n",
    "\n",
    "age_std = sc_std.fit_transform(df_imp[['Age']])\n",
    "fare_mm = sc_mm.fit_transform(df_imp[['Fare']])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(age_std.flatten(), bins=30)\n",
    "axes[0].set_title('Age - Standardized')\n",
    "axes[1].hist(fare_mm.flatten(), bins=30)\n",
    "axes[1].set_title('Fare - MinMax [0,1]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d456b1",
   "metadata": {},
   "source": [
    "**üìù Exercise 3**\n",
    "1) Standardize `Fare` and plot the histogram.  \n",
    "2) Apply **Normalizer** on `[Age, Fare]` rows and check the first 5 normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "df_small = df[['Age','Fare']].dropna().copy()\n",
    "\n",
    "normalizer = Normalizer()\n",
    "norm_data = normalizer.fit_transform(df_small)\n",
    "\n",
    "\n",
    "norm_df = pd.DataFrame(norm_data, columns=['Age_norm','Fare_norm'])\n",
    "\n",
    "\n",
    "print(norm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")   # adjust path\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['Fare_std'] = scaler.fit_transform(df[['Fare']])\n",
    "\n",
    "\n",
    "plt.hist(df['Fare_std'], bins=30, edgecolor='black')\n",
    "plt.title(\"Standardized Fare Distribution\")\n",
    "plt.xlabel(\"Standardized Fare\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2e146",
   "metadata": {},
   "source": [
    "## 4) Encoding Categorical Variables (Individually)\n",
    "\n",
    "- **Ordinal/Label encoding**: map categories to integers (assumes order or used with tree models).  \n",
    "- **One-Hot encoding**: binary column per category (no order assumption).\n",
    "\n",
    "We'll use `Sex` and `Embarked` as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a961c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Ordinal encoding demo (note: no real order in Sex/Embarked; this is just to illustrate)\n",
    "enc = OrdinalEncoder()\n",
    "ord_demo = df_imp[['Sex','Embarked']].copy()\n",
    "ord_vals = enc.fit_transform(ord_demo)\n",
    "pd.DataFrame(ord_vals, columns=['Sex_ord','Embarked_ord']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 One-Hot encoding demo with pandas\n",
    "ohe_embarked = pd.get_dummies(df_imp['Embarked'], prefix='Embarked')\n",
    "ohe_sex = pd.get_dummies(df_imp['Sex'], prefix='Sex')\n",
    "encoded_df = pd.concat([df_imp[['Survived','Age','Fare']], ohe_sex, ohe_embarked], axis=1)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0cd1f",
   "metadata": {},
   "source": [
    "**üìù Exercise 4**\n",
    "1) Compare the **number of features** produced by ordinal vs one-hot for `Embarked`.  \n",
    "2) Why might one-hot be safer for linear models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40ee54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "670e73e6",
   "metadata": {},
   "source": [
    "Ordinal encoding introduces a false sense of order.\n",
    "With ordinal encoding, \"S\" (2) is ‚Äúgreater than‚Äù \"Q\" (1) which is ‚Äúgreater than‚Äù \"C\" (0).\n",
    "A linear regression or logistic regression will treat this as numeric distance, e.g., assume \"S\" is twice \"Q\", which is meaningless.\n",
    "This can distort coefficients and model interpretation.\n",
    "One-hot avoids this by treating categories as independent, non-ordered indicators.\n",
    "\"C\" = [1,0,0], \"Q\" = [0,1,0], \"S\" = [0,0,1].\n",
    "The model assigns separate weights, no fake ordering.\n",
    "That‚Äôs why one-hot is safer for linear models (and often tree models too, though trees can sometimes handle ordinal codes fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fc515",
   "metadata": {},
   "source": [
    "Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "df['Embarked_ord'] = ord_enc.fit_transform(df[['Embarked']])\n",
    "Categories mapped to numbers, e.g. C=0, Q=1, S=2.\n",
    "\n",
    "One-Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop=None, sparse=False)\n",
    "embarked_ohe = ohe.fit_transform(df[['Embarked']])\n",
    "Creates separate binary columns for each category: Embarked_C, Embarked_Q, Embarked_S.\n",
    "Produces 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c161dfa",
   "metadata": {},
   "source": [
    "## 5) Feature Transformation (Individually)\n",
    "\n",
    "- **Log transform**: t = log1p(x) for right-skewed positive data (e.g., Fare).\n",
    "- **Power transform**: Yeo-Johnson can handle zero/negative values; stabilizes variance.\n",
    "- **Polynomial features**: create interactions/quadratics for simple non-linear modeling.\n",
    "\n",
    "We'll use `Fare` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5012b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Log transform on Fare (positive values)\n",
    "fare_raw = df_imp['Fare'].dropna().values.reshape(-1,1)\n",
    "fare_log = np.log1p(fare_raw)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(fare_raw.flatten(), bins=30)\n",
    "axes[0].set_title('Fare - Raw')\n",
    "axes[1].hist(fare_log.flatten(), bins=30)\n",
    "axes[1].set_title('Fare - log1p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c34a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Power transform (Yeo-Johnson) on [Age, Fare]\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "af = df_imp[['Age','Fare']].dropna()\n",
    "af_pt = pt.fit_transform(af)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(af['Age'].values, bins=30)\n",
    "axes[0].set_title('Age - Raw')\n",
    "axes[1].hist(af_pt[:,0], bins=30)\n",
    "axes[1].set_title('Age - Yeo-Johnson')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Polynomial features on [Age, Fare] (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "af_poly = poly.fit_transform(af[['Age','Fare']])\n",
    "print(\"Original shape:\", af[['Age','Fare']].shape, \" -> With poly:\", af_poly.shape)\n",
    "poly.get_feature_names_out(['Age','Fare'])[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f15002",
   "metadata": {},
   "source": [
    "**üìù Exercise 5**\n",
    "1) Identify one numeric column that is **skewed**. Try both **log** and **power** transforms and compare histograms.  \n",
    "2) With `PolynomialFeatures(2)`, which new terms are created from `Age` and `Fare`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(df[['Age','Fare']].dropna())\n",
    "\n",
    "print(poly.get_feature_names_out(['Age','Fare']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "fare = df['Fare'].dropna()\n",
    "\n",
    "\n",
    "fare_log = np.log1p(fare)\n",
    "\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "fare_power = pt.fit_transform(fare.values.reshape(-1,1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(fare, bins=30, edgecolor='black')\n",
    "plt.title(\"Original Fare\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(fare_log, bins=30, edgecolor='black')\n",
    "plt.title(\"Log(Fare+1)\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(fare_power, bins=30, edgecolor='black')\n",
    "plt.title(\"Power Transform (Yeo-Johnson)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b550f5f",
   "metadata": {},
   "source": [
    "## 6) Simple Dimensionality Reduction (PCA) ‚Äî Visualization Only\n",
    "\n",
    "We will apply PCA to **numeric** features to reduce to 2D and make a scatter plot colored by `Survived` (if present).\n",
    "\n",
    "> Note: This is for **intuition/visualization** only in Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_only = df_imp.select_dtypes(include=['number']).dropna()\n",
    "y = df_imp.loc[num_only.index, 'Survived'] if 'Survived' in df_imp.columns else None\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Z = pca.fit_transform(num_only.values)\n",
    "\n",
    "print(\"Explained variance ratios:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "if y is not None:\n",
    "   \n",
    "    idx0 = (y.values == 0)\n",
    "    idx1 = (y.values == 1)\n",
    "    plt.scatter(Z[idx0,0], Z[idx0,1], s=10, label='Survived=0')\n",
    "    plt.scatter(Z[idx1,0], Z[idx1,1], s=10, label='Survived=1')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.scatter(Z[:,0], Z[:,1], s=10)\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (numeric only)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade92af5",
   "metadata": {},
   "source": [
    "**üìù Exercise 6**\n",
    "1) Which **two numeric columns** contribute the most variance before PCA (use `df.var()`)?  \n",
    "2) Try PCA with `n_components=3` and print the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale first (important for PCA)\n",
    "X_scaled = StandardScaler().fit_transform(num_cols.dropna())\n",
    "\n",
    "# PCA with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Cumulative explained variance\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = df.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "variances = num_cols.var().sort_values(ascending=False)\n",
    "print(variances)\n",
    "\n",
    "\n",
    "print(\"Top 2 columns by variance:\", variances.index[:2].tolist())\n",
    "\n",
    "#Fare and Age contribute the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52dd9c",
   "metadata": {},
   "source": [
    "## 7) Consolidated Practice (No Pipelines Yet)\n",
    "\n",
    "Using the operations you've learned, perform a **clean preprocessing** (manually):\n",
    "1) Impute: `Age` (median), `Embarked` (mode), `Cabin` ('Unknown').  \n",
    "2) Scale: standardize `Age` and min-max scale `Fare`.  \n",
    "3) Encode: one-hot `Sex` and `Embarked`.  \n",
    "4) Transform: log1p `Fare`.  \n",
    "5) (Optional) PCA on numeric subset for 2D visualization.\n",
    "\n",
    "Then, answer:\n",
    "- Which step **changed the data distribution** the most?\n",
    "- Which encoding produced **more features**, ordinal or one-hot? Why?\n",
    "- If you trained a simple logistic regression on your manually processed features, what **accuracy** do you get on a 75/25 split? (Optional challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())       \n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0]) \n",
    "df['Cabin'] = df['Cabin'].fillna('Unknown')            \n",
    "\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "df['Age_std'] = scaler_std.fit_transform(df[['Age']])  \n",
    "\n",
    "scaler_mm = MinMaxScaler()\n",
    "df['Fare_mm'] = scaler_mm.fit_transform(df[['Fare']])  \n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(drop=None, sparse=False)\n",
    "sex_embarked = ohe.fit_transform(df[['Sex','Embarked']])\n",
    "\n",
    "ohe_cols = ohe.get_feature_names_out(['Sex','Embarked'])\n",
    "df_ohe = pd.DataFrame(sex_embarked, columns=ohe_cols, index=df.index)\n",
    "\n",
    "df = pd.concat([df, df_ohe], axis=1)\n",
    "\n",
    "\n",
    "df['Fare_log'] = np.log1p(df['Fare']) \n",
    "\n",
    "\n",
    "num_subset = df[['Age_std','Fare_mm','Fare_log']].dropna()\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(num_subset)\n",
    "\n",
    "print(\"PCA explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "\n",
    "X = pd.concat([df[['Age_std','Fare_mm','Fare_log']], df_ohe], axis=1)\n",
    "y = df['Survived']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78320516",
   "metadata": {},
   "source": [
    "\n",
    "1. The log transform on Fare changed the distribution the most, because Fare is highly skewed and log1p compresses large values, making the histogram more ‚Äúnormal‚Äù.\n",
    "\n",
    "2. One-hot encoding produced more features. Ordinal encoding uses only 1 column per category, but one-hot creates separate binary columns for each category to avoid introducing false order. For Sex (2 classes) and Embarked (3 classes), one-hot gave 5 features.\n",
    "\n",
    "3. Around 0.78‚Äì0.82 depending on random state (typical Titanic baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7413b3",
   "metadata": {},
   "source": [
    "## ‚úÖ What You Should Take Away from Part A\n",
    "\n",
    "- Each step (imputation, scaling, encoding, transforms) has a **clear purpose** and **visible effect**.  \n",
    "- You can now apply them **manually** and reason about their impact.  \n",
    "- Next: move to **Part B (Pipelines)** to **combine & automate** these steps safely (avoid leakage, enable cross-validation, and reproducibility)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
